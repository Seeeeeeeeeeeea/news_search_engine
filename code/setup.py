# -*- coding: utf-8 -*-
"""
Updated Setup Script (2025)
ä½œè€…: æ ¹æ®ä½ çš„åŸç‰ˆä¿®æ”¹
"""

from spider import get_news_pool
from spider import crawl_news
from index_module import IndexModule
from recommendation_module import RecommendationModule
from datetime import datetime
import urllib.request
import configparser
import os
import sys

# ------------------ è·å–ç½‘é¡µæœ€å¤§é¡µæ•° ------------------
def get_max_page(url, try_auto=True):
    """
    try_auto=Trueï¼šå¦‚æœç½‘é¡µåŒ…å« maxPage ä¼šè‡ªåŠ¨è§£æ
    å¦åˆ™è¿”å› Noneï¼Œéœ€è¦æ‰‹åŠ¨æŒ‡å®š
    """
    try:
        response = urllib.request.urlopen(url)
        html = response.read().decode("utf-8", "ignore")

        if try_auto and "maxPage" in html:
            part = html[html.find('maxPage'):]
            part = part[:part.find(';')]
            return int(part[part.find('=')+1:].strip())
        return None
    except Exception as e:
        print(f"âš  maxPage è§£æå¤±è´¥ï¼š {e}")
        return None


# ------------------ çˆ¬å–æ–°é—» ------------------
def crawling(config):
    print(f"\n===============================================\nå¯åŠ¨æ—¶é—´: {datetime.today()}\n===============================================\n")

    # ================== ä¿®æ”¹è¿™é‡Œ ==================
    root_url = "https://news.baidu.com"       # â† ä¸»ç«™
    list_url = "https://news.baidu.com/guonei?pn="  # â† å›½å†…æ–°é—»ç¿»é¡µ URL
    max_page = get_max_page(root_url + ".shtml") or 10  # é»˜è®¤ 10 é¡µ
    print(f"ğŸ“„ æœ€å¤§çˆ¬å–é¡µæ•°ï¼š{max_page}")
    # =============================================

    # æ„å»ºæ–°é—» URL æ± 
    news_pool = [list_url + str(i * 20) for i in range(max_page)]

    # çˆ¬å–æ–°é—»
    crawl_news(
        news_pool,
        140,   # çˆ¬å–æ–°é—»æ¡æ•°
        config['doc_dir_path'],
        config['doc_encoding']
    )
    print("ğŸŸ© æ–°é—»çˆ¬å–å®Œæˆ\n")


# ------------------ ä¸»ç¨‹åº ------------------
if __name__ == "__main__":
    # é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆç»å¯¹è·¯å¾„ï¼‰
    config_path = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "config.ini"))
    if not os.path.exists(config_path):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        sys.exit(1)

    # è¯»å–é…ç½®
    config_parser = configparser.ConfigParser()
    read_files = config_parser.read(config_path, encoding="utf-8")
    if not read_files:
        print(f"âŒ é…ç½®æ–‡ä»¶è¯»å–å¤±è´¥: {config_path}")
        sys.exit(1)
    config = config_parser['DEFAULT']

    # æ‰“å°è¯»å–æƒ…å†µï¼ˆè°ƒè¯•ç”¨ï¼‰
    print("ğŸŸ¢ æ­£åœ¨åŠ è½½é…ç½®æ–‡ä»¶:", config_path)
    print("é»˜è®¤å­—æ®µ:", dict(config))

    # çˆ¬æ–°é—»
    crawling(config)

    # å»ºç«‹ç´¢å¼•
    print("ğŸ” å¼€å§‹å»ºç«‹ç´¢å¼•...")
    im = IndexModule(config_path, "utf-8")
    im.construct_postings_lists()

    # æ¨èé˜…è¯»
    print("ğŸ” å¼€å§‹æ¨èæ–°é—»...")
    rm = RecommendationModule(config_path, "utf-8")
    rm.find_k_nearest(5, 25)

    print(f"===============================================\nå®Œæˆæ—¶é—´: {datetime.today()}\n===============================================\n")
